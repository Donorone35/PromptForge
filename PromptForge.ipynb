{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, we will use the following libraries:\n",
    "\n",
    "*   [`ibm-watsonx-ai`](https://ibm.github.io/watson-machine-learning-sdk/index.html): Enables the use of LLMs from IBM's watsonx.ai.\n",
    "*   [`langchain`](https://www.langchain.com/): Provides various chain and prompt functions from LangChain.\n",
    "*   [`langchain-ibm`](https://python.langchain.com/v0.1/docs/integrations/llms/ibm_watsonx/): Facilitates integration between LangChain and IBM watsonx.ai.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install \"ibm-watsonx-ai==1.0.8\" --user\n",
    "!pip install \"langchain==0.2.11\" --user\n",
    "!pip install \"langchain-ibm==0.1.7\" --user\n",
    "!pip install \"langchain-core==0.2.43\" --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# IBM WatsonX imports\n",
    "from ibm_watsonx_ai.foundation_models import Model\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes\n",
    "\n",
    "from langchain_ibm import WatsonxLLM\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableSequence\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain.chains import LLMChain  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "from copy import deepcopy\n",
    "from langchain_ibm import WatsonxLLM\n",
    "\n",
    "# allowed Watsonx generation params\n",
    "ALLOWED_PARAMS = {\n",
    "    \"decoding_method\", \"temperature\", \"top_p\", \"top_k\",\n",
    "    \"random_seed\", \"repetition_penalty\",\n",
    "    \"min_new_tokens\", \"max_new_tokens\",\n",
    "    \"length_penalty\", \"truncate_input_tokens\",\n",
    "    \"stop_sequences\", \"prompt_variables\",\n",
    "    \"return_options\"\n",
    "}\n",
    "\n",
    "def llm_model(prompt_txt, task=\"qa\", params=None, log=True, log_file=\"llm_logs.jsonl\"):\n",
    "    \"\"\"\n",
    "    Invoke IBM Granite LLM with task-specific default parameters.\n",
    "\n",
    "    Args:\n",
    "        prompt_txt (str): The input text or question.\n",
    "        task (str): One of [\"summarization\", \"qa\", \"classification\", \"code\", \"roleplay\"].\n",
    "        params (dict): Optional overrides for generation parameters.\n",
    "        log (bool): Whether to log request/response metadata.\n",
    "        log_file (str): File path for logs (JSONL format).\n",
    "    \"\"\"\n",
    "\n",
    "    model_id = \"ibm/granite-3-2-8b-instruct\"\n",
    "    url = \"https://us-south.ml.cloud.ibm.com\"\n",
    "    project_id = \"skills-network\"\n",
    "\n",
    "    # --------------------------\n",
    "    # Task-specific default params\n",
    "    # --------------------------\n",
    "    task_params = {\n",
    "        \"summarization\": {\n",
    "            \"decoding_method\": \"sample\",\n",
    "            \"max_new_tokens\": 200,\n",
    "            \"min_new_tokens\": 30,\n",
    "            \"temperature\": 0.3,\n",
    "            \"top_p\": 0.4,\n",
    "            \"repetition_penalty\": 1.2,\n",
    "            \"length_penalty\": {\"decay_factor\": 2.0, \"start_index\": 10},\n",
    "            \"return_options\": {\"input_text\": True, \"generated_tokens\": True}\n",
    "        },\n",
    "        \"qa\": {\n",
    "            \"decoding_method\": \"sample\",\n",
    "            \"max_new_tokens\": 256,\n",
    "            \"min_new_tokens\": 10,\n",
    "            \"temperature\": 0.4,\n",
    "            \"top_p\": 0.3,\n",
    "            \"top_k\": 5,\n",
    "            \"repetition_penalty\": 1.1,\n",
    "            \"return_options\": {\"input_text\": True, \"generated_tokens\": True, \"token_logprobs\": True}\n",
    "        },\n",
    "        \"classification\": {\n",
    "            \"decoding_method\": \"greedy\",\n",
    "            \"max_new_tokens\": 50,\n",
    "            \"min_new_tokens\": 1,\n",
    "            \"temperature\": 0.2,\n",
    "            \"top_p\": 0.2,\n",
    "            \"top_k\": 1,\n",
    "            \"repetition_penalty\": 1.0,\n",
    "            \"return_options\": {\"input_text\": True, \"generated_tokens\": True}\n",
    "        },\n",
    "        \"code\": {\n",
    "            \"decoding_method\": \"sample\",\n",
    "            \"max_new_tokens\": 300,\n",
    "            \"min_new_tokens\": 20,\n",
    "            \"temperature\": 0.3,\n",
    "            \"top_p\": 0.3,\n",
    "            \"top_k\": 20,\n",
    "            \"repetition_penalty\": 1.2,\n",
    "            # user can override stop_sequences if needed\n",
    "            \"return_options\": {\"input_text\": True, \"generated_tokens\": True}\n",
    "        },\n",
    "        \"roleplay\": {\n",
    "            \"decoding_method\": \"sample\",\n",
    "            \"max_new_tokens\": 256,\n",
    "            \"min_new_tokens\": 50,\n",
    "            \"temperature\": 0.8,\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": 40,\n",
    "            \"repetition_penalty\": 1.0,\n",
    "            \"return_options\": {\"input_text\": True, \"generated_tokens\": True}\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # pick defaults for task\n",
    "    default_params = deepcopy(task_params.get(task, task_params[\"qa\"]))\n",
    "\n",
    "    # merge overrides if provided (validated)\n",
    "    if params:\n",
    "        clean_params = {k: v for k, v in params.items() if k in ALLOWED_PARAMS}\n",
    "        default_params.update(clean_params)\n",
    "\n",
    "    # Initialize Granite LLM\n",
    "    granite_llm = WatsonxLLM(\n",
    "        model_id=model_id,\n",
    "        project_id=project_id,\n",
    "        url=url,\n",
    "        params=default_params\n",
    "    )\n",
    "\n",
    "    # Invoke model\n",
    "    response = granite_llm.invoke(prompt_txt)\n",
    "\n",
    "    # --------------------------\n",
    "    # Optional logging\n",
    "    # --------------------------\n",
    "    if log:\n",
    "        log_entry = {\n",
    "            \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "            \"task\": task,\n",
    "            \"prompt\": prompt_txt,\n",
    "            \"params\": default_params,\n",
    "            \"response\": str(response)\n",
    "        }\n",
    "        with open(log_file, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps(log_entry) + \"\\n\")\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'decoding_method': 'sample', 'length_penalty': {'decay_factor': 2.5, 'start_index': 5}, 'temperature': 0.5, 'top_p': 0.2, 'top_k': 1, 'random_seed': 33, 'repetition_penalty': 2, 'min_new_tokens': 50, 'max_new_tokens': 200, 'stop_sequences': ['fail'], ' time_limit': 600000, 'truncate_input_tokens': 200, 'prompt_variables': {'object': 'brain'}, 'return_options': {'input_text': True, 'generated_tokens': True, 'input_tokens': True, 'token_logprobs': True, 'token_ranks': False, 'top_n_tokens': False}}\n"
     ]
    }
   ],
   "source": [
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "\n",
    "print(GenParams().get_example_values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the rocky planets in the solar system?\n",
      "\n",
      "The four terrestrial or rocky planets in our solar system are Mercury, Venus, Earth, and Mars. These planets share several characteristics: they have solid surfaces, relatively small sizes compared to gas giants, and are composed primarily of silicate rocks and metals. Here's a brief overview of each:\n",
      "\n",
      "1. **Mercury**: The smallest planet in our solar system, Mercury is closest to the Sun. It has a heavily cratered surface similar to Earth's Moon due to its lack of atmosphere and geological activity. Its thin exosphere contains oxygen, sodium, hydrogen, helium, and potassium.\n",
      "\n",
      "2. **Venus**: Often called Earth's \"sister planet\" because of their similar size, Venus is the second planet from the Sun. However, it has a very different environment. Venus has a thick toxic atmosphere composed mainly of carbon dioxide with clouds of sulfuric acid. Its surface temperature can reach up to 900 degrees Fahrenheit (475 degrees Celsius), making it the hottest planet in our solar system\n"
     ]
    }
   ],
   "source": [
    "print(llm_model(\"What are the rocky planets in the solar system?\", task=\"qa\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize: Artificial intelligence is transforming industries like healthcare and finance. In healthcare, AI can analyze medical images to detect diseases earlier than traditional methods, improving patient outcomes. It also assists in drug discovery by predicting how different compounds will interact with the human body.\n",
      "\n",
      "In finance, AI algorithms are used for fraud detection, risk assessment, and algorithmic trading. They can process vast amounts of data quickly, identifying patterns that humans might miss. This leads to more accurate predictions and better decision-making. However, ethical concerns arise around privacy, job displacement due to automation, and potential biases in AI systems if not properly trained or monitored.\"\n"
     ]
    }
   ],
   "source": [
    "print(llm_model(\n",
    "    \"Summarize: Artificial intelligence is transforming industries like healthcare and finance.\",\n",
    "    task=\"summarization\"\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add some prompt templates to check the model performance for predefined prompt styles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Q&A template\n",
    "qa_template = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"You are a helpful AI. Answer the following question clearly:\\n{question}\"\n",
    ")\n",
    "\n",
    "# Summarization template\n",
    "summ_template = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"Summarize the following text in 3 bullet points:\\n{text}\"\n",
    ")\n",
    "\n",
    "# Roleplay template\n",
    "roleplay_template = PromptTemplate(\n",
    "    input_variables=[\"scenario\"],\n",
    "    template=\"Pretend you are a {scenario} and respond accordingly.\"\n",
    ")\n",
    "\n",
    "# Classification template\n",
    "classify_template = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"Classify the sentiment of the text as Positive, Neutral, or Negative:\\n{text}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful AI. Answer the following question clearly:\n",
      "What is the capital of France?\n",
      "\n",
      "The capital of France is Paris.\n",
      "Summarize the following text in 3 bullet points:\n",
      "AI is rapidly transforming healthcare...[in] drug discovery and development, medical imaging analysis, patient monitoring, personalized medicine, and more. Here are some key areas where AI shines:\n",
      "1. **Drug Discovery**: AI can analyze vast amounts of data to identify potential new drugs or repurpose existing ones for different uses. It accelerates this process by predicting how compounds will behave and interact with biological systems.\n",
      "2. **Medical Imaging Analysis**: AI algorithms excel at interpreting complex images like MRIs and CT scans, often matching or surpassing human experts in accuracy while reducing diagnosis time. They can detect subtle patterns indicative of diseases such as cancer, Alzheimer's, or heart conditions.\n",
      "3. **Personalized Medicine**: By analyzing a patient's genetic information alongside clinical data, AI can tailor treatments specifically to individual patients, improving efficacy and minimizing side effects. This approach\n"
     ]
    }
   ],
   "source": [
    "prompt = qa_template.format(question=\"What is the capital of France?\")\n",
    "response = llm_model(prompt, task=\"qa\")\n",
    "print(response)\n",
    "\n",
    "prompt = summ_template.format(text=\"AI is rapidly transforming healthcare...\")\n",
    "response = llm_model(prompt, task=\"summarization\")\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- QA ---\n",
      "You are a helpful AI. Answer the following question clearly:\n",
      "What is the capital of France?\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      "--- SUMMARIZATION ---\n",
      "Summarize the following text in 3 bullet points:\n",
      "AI is rapidly transforming healthcare...[in] drug discovery and development, medical imaging analysis, patient monitoring, personalized medicine, and more. Here are some key areas where AI shines:\n",
      "1. **Drug Discovery**: AI can analyze vast amounts of data to identify potential new drugs or repurpose existing ones for different uses. It accelerates this process by predicting how compounds will behave and interact with biological systems.\n",
      "2. **Medical Imaging Analysis**: AI algorithms excel at interpreting complex images like MRIs and CT scans, often matching or surpassing human experts in accuracy while reducing diagnosis time. They can detect subtle patterns indicative of diseases such as cancer, Alzheimer's, or heart conditions.\n",
      "3. **Personalized Medicine**: By analyzing a patient's genetic information alongside clinical data, AI can tailor treatments specifically to individual patients, improving efficacy and minimizing side effects. This approach\n",
      "\n",
      "--- ROLEPLAY ---\n",
      "Pretend you are a friendly tour guide in Paris and respond accordingly.\n",
      "\n",
      "Bonjour et bienvenue à Paris, la Ville Lumière! I'm thrilled to be your guide as we explore the enchanting streets and iconic landmarks of this magical city. Let's begin our journey at the magnificent Eiffel Tower.\n",
      "\n",
      "As we approach, marvel at its iron lattice structure standing tall against the skyline. Designed by Gustave Eiffel for the 1889 World's Fair, it was initially criticized but has since become an enduring symbol of France and one of the most recognizable structures in the world. Don't forget to take a moment to appreciate the tower's intricate design and the stunning views from the top.\n",
      "\n",
      "Next, let's stroll along the Champs-Élysées, Paris' most famous avenue. This grand boulevard stretches over a mile and is lined with luxury shops, cafes, and cinemas. At its eastern end, you'll find the Arc de Triomphe, a monument honoring those who fought and died for France\n",
      "\n",
      "--- CLASSIFICATION ---\n",
      "Classify the sentiment of the following text as Positive, Negative, or Neutral:\n",
      "I am so excited about my new job! I can't wait to start and meet my new colleagues.\n",
      "\n",
      "Positive\n",
      "\n",
      "Here are step-by-step reasoning steps that can be used to reach the final answers.\n",
      "\n",
      "\n",
      "\n",
      "Step 1: Identify the sentiment\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# --------------------------\n",
    "# Multiple Prompt Templates\n",
    "# --------------------------\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "qa_template = PromptTemplate.from_template(\n",
    "    \"You are a helpful AI. Answer the following question clearly:\\n{question}\"\n",
    ")\n",
    "\n",
    "summ_template = PromptTemplate.from_template(\n",
    "    \"Summarize the following text in 3 bullet points:\\n{text}\"\n",
    ")\n",
    "\n",
    "roleplay_template = PromptTemplate.from_template(\n",
    "    \"Pretend you are a {scenario} and respond accordingly.\"\n",
    ")\n",
    "\n",
    "classify_template = PromptTemplate.from_template(\n",
    "    \"Classify the sentiment of the following text as Positive, Negative, or Neutral:\\n{text}\"\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Test prompts for evaluation\n",
    "# --------------------------\n",
    "tests = [\n",
    "    (\"qa\", qa_template.format(question=\"What is the capital of France?\")),\n",
    "    (\"summarization\", summ_template.format(text=\"AI is rapidly transforming healthcare...\")),\n",
    "    (\"roleplay\", roleplay_template.format(scenario=\"friendly tour guide in Paris\")),\n",
    "    (\"classification\", classify_template.format(text=\"I am so excited about my new job!\"))\n",
    "]\n",
    "\n",
    "# --------------------------\n",
    "# JSONL logging setup\n",
    "# --------------------------\n",
    "log_file = \"prompt_eval_log.jsonl\"\n",
    "\n",
    "def log_jsonl(entry, file_path=log_file):\n",
    "    with open(file_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# --------------------------\n",
    "# Run evaluation loop\n",
    "# --------------------------\n",
    "for task, prompt in tests:\n",
    "    print(f\"\\n--- {task.upper()} ---\")\n",
    "    response = llm_model(prompt, task=task, log=False)  # disable inner logging here\n",
    "    \n",
    "    # Print to console\n",
    "    print(response)\n",
    "\n",
    "    # Log structured data\n",
    "    entry = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"task\": task,\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": response\n",
    "    }\n",
    "    log_jsonl(entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "92bcc35e85653c09624c5a3378dfa8f49118b6e7728ecd05617d28b9ca1364f4"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
